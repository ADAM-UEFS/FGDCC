INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:root:called-params configs/in1k_vith14_ep300_deeper_cluster.yaml
INFO:root:loaded params...
{   'data': {   'batch_size': 64,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 0.0,
                'drop_path': 0.25,
                'image_folder': '/home/rtcalumby/adam/luciano/plantnet_300K/',
                'mixup': 0.0,
                'nb_classes': 1081,
                'num_workers': 10,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/PlantNet300k',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 50,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 0.0001,
                        'start_lr': 0.0001,
                        'warmup': 10,
                        'weight_decay': 0.05}}
INFO:root:Running... (rank: 0/1)
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:AutoEncoder(
  (encoder): Sequential(
    (0): Linear(in_features=1280, out_features=1024, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=1024, out_features=768, bias=True)
    (3): GELU(approximate='none')
    (4): Linear(in_features=768, out_features=512, bias=True)
    (5): GELU(approximate='none')
    (6): Linear(in_features=512, out_features=256, bias=True)
    (7): GELU(approximate='none')
  )
  (decoder): Sequential(
    (0): Linear(in_features=256, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=768, bias=True)
    (3): GELU(approximate='none')
    (4): Linear(in_features=768, out_features=1024, bias=True)
    (5): GELU(approximate='none')
    (6): Linear(in_features=1024, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.25, inplace=False)
  (out): Sigmoid()
)
INFO:root:making imagenet data transforms
INFO:root:making imagenet data transforms
INFO:root:Finetuning dataset created
Training dataset, length: 243904
INFO:root:Finetuning dataset created
Val dataset, length: 31104
INFO:root:Using AdamW
INFO:root:loaded pretrained encoder from epoch 66 with msg: <All keys matched successfully>
INFO:root:loaded pretrained encoder from epoch 66 with msg: <All keys matched successfully>
['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 66 with msg: <All keys matched successfully>
INFO:root:loaded optimizers from epoch 66
INFO:root:read-path: /home/rtcalumby/adam/luciano/LifeCLEFPlant2022/IN22K-vit.h.14-900e.pth.tar
INFO:root:Using AdamW
INFO:root:DistributedDataParallel(
  (module): FinetuningModel(
    (pretrained_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))
      )
      (blocks): ModuleList(
        (0-31): 32 x Block(
          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=1280, out_features=5120, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=5120, out_features=1280, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    )
    (head_drop): Dropout(p=0.25, inplace=False)
    (hierarchical_classifier): HierarchicalClassifier(
      (parent_classifier): ParentClassifier(
        (fc): Linear(in_features=1280, out_features=1081, bias=True)
      )
      (child_classifiers): ModuleList(
        (0): ModuleList(
          (0-1080): 1081 x ChildClassifier(
            (fc): Linear(in_features=1280, out_features=2, bias=True)
          )
        )
        (1): ModuleList(
          (0-1080): 1081 x ChildClassifier(
            (fc): Linear(in_features=1280, out_features=3, bias=True)
          )
        )
        (2): ModuleList(
          (0-1080): 1081 x ChildClassifier(
            (fc): Linear(in_features=1280, out_features=4, bias=True)
          )
        )
        (3): ModuleList(
          (0-1080): 1081 x ChildClassifier(
            (fc): Linear(in_features=1280, out_features=5, bias=True)
          )
        )
      )
    )
  )
)
INFO:root:Epoch 1
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
Process Process-1:
Traceback (most recent call last):
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/main_deeper_cluster.py", line 52, in process_main
    app_main(args=params)
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/engine_deeper_cluster.py", line 501, in main
    (loss, reconstruction_loss, _new_lr, _new_wd, grad_stats, bottleneck_output), etime = gpu_timer(train_step)
                                                                                          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/src/utils/logging.py", line 21, in gpu_timer
    result = closure()
             ^^^^^^^^^
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/engine_deeper_cluster.py", line 441, in train_step
    k_means_loss, k_means_assignments = k_means_module.assign(bottleneck_output, targets, device, cached_features_last_iter)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/src/KMeans.py", line 130, in assign
    self.n_kmeans[y[i]][k].train(batch_x)
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/site-packages/faiss/extra_wrappers.py", line 544, in train
    x = np.ascontiguousarray(x, dtype='float32')
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/site-packages/torch/_tensor.py", line 1089, in __array__
    return self.numpy().astype(dtype, copy=False)
           ^^^^^^^^^^^^
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
Reconstruction loss: tensor(0.5901, device='cuda:0', grad_fn=<SmoothL1LossBackward0>)
